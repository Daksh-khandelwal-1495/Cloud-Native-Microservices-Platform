# KEDA Installation and Configuration for Event-Driven Autoscaling

## This setup creates a complete event-driven scaling system using KEDA with Redis queues

---
# Redis Queue Service for Event-Driven Scaling
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-queue
  namespace: database
  labels:
    app: redis-queue
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis-queue
  template:
    metadata:
      labels:
        app: redis-queue
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        ports:
        - containerPort: 6379
        args:
        - redis-server
        - --appendonly
        - "yes"
        - --maxmemory
        - 256mb
        - --maxmemory-policy
        - allkeys-lru
        resources:
          requests:
            memory: "128Mi"
            cpu: "50m"
          limits:
            memory: "256Mi"
            cpu: "200m"
        volumeMounts:
        - name: redis-storage
          mountPath: /data
      volumes:
      - name: redis-storage
        emptyDir: {}

---
apiVersion: v1
kind: Service
metadata:
  name: redis-queue
  namespace: database
  labels:
    app: redis-queue
spec:
  selector:
    app: redis-queue
  ports:
    - port: 6379
      targetPort: 6379
  type: ClusterIP

---
# KEDA ScaledObject for User Service based on Redis Queue Length
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: user-service-keda-scaler
  namespace: default
spec:
  scaleTargetRef:
    name: user-service
  pollingInterval: 10
  cooldownPeriod: 60
  minReplicaCount: 1
  maxReplicaCount: 15
  triggers:
  - type: redis
    metadata:
      address: redis-queue.database:6379
      listName: user-queue
      listLength: "5"
      enableTLS: "false"
  - type: prometheus
    metadata:
      serverAddress: http://prometheus.monitoring:9090
      metricName: predicted_cpu_utilization
      threshold: "60"
      query: predicted_cpu_utilization{service="user-service"}

---
# KEDA ScaledObject for Catalog Service
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: catalog-service-keda-scaler
  namespace: default
spec:
  scaleTargetRef:
    name: catalog-service
  pollingInterval: 10
  cooldownPeriod: 60
  minReplicaCount: 1
  maxReplicaCount: 12
  triggers:
  - type: redis
    metadata:
      address: redis-queue.database:6379
      listName: catalog-queue
      listLength: "3"
      enableTLS: "false"
  - type: prometheus
    metadata:
      serverAddress: http://prometheus.monitoring:9090
      metricName: predicted_cpu_utilization
      threshold: "55"
      query: predicted_cpu_utilization{service="catalog-service"}

---
# KEDA ScaledObject for Order Service
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: order-service-keda-scaler
  namespace: default
spec:
  scaleTargetRef:
    name: order-service
  pollingInterval: 15
  cooldownPeriod: 90
  minReplicaCount: 2
  maxReplicaCount: 20
  triggers:
  - type: redis
    metadata:
      address: redis-queue.database:6379
      listName: order-queue
      listLength: "2"
      enableTLS: "false"
  - type: prometheus
    metadata:
      serverAddress: http://prometheus.monitoring:9090
      metricName: predicted_cpu_utilization
      threshold: "50"
      query: predicted_cpu_utilization{service="order-service"}
  - type: prometheus
    metadata:
      serverAddress: http://prometheus.monitoring:9090
      metricName: flask_http_request_total
      threshold: "100"
      query: sum(rate(flask_http_request_total{service="order-service"}[1m])) * 60

---
# Queue Producer Service for Testing Event-Driven Scaling
apiVersion: apps/v1
kind: Deployment
metadata:
  name: queue-producer
  namespace: default
  labels:
    app: queue-producer
spec:
  replicas: 1
  selector:
    matchLabels:
      app: queue-producer
  template:
    metadata:
      labels:
        app: queue-producer
    spec:
      containers:
      - name: producer
        image: redis:7-alpine
        command: 
        - /bin/sh
        - -c
        - |
          apk add --no-cache python3 py3-pip
          pip3 install redis
          cat > /app/producer.py << 'EOF'
          import redis
          import time
          import random
          import json
          from datetime import datetime
          
          r = redis.Redis(host='redis-queue.database', port=6379, decode_responses=True)
          queues = ['user-queue', 'catalog-queue', 'order-queue']
          
          print("Queue producer started...")
          
          while True:
              for queue in queues:
                  # Simulate varying load patterns
                  hour = datetime.now().hour
                  if 8 <= hour <= 18:  # Business hours
                      messages = random.randint(1, 8)
                  else:  # Off hours
                      messages = random.randint(0, 3)
                  
                  for i in range(messages):
                      message = {
                          'timestamp': datetime.now().isoformat(),
                          'queue': queue,
                          'task_id': f"{queue}_{int(time.time())}_{i}",
                          'priority': random.choice(['high', 'medium', 'low'])
                      }
                      r.lpush(queue, json.dumps(message))
                      print(f"Added message to {queue}: {message['task_id']}")
              
              # Wait before next batch
              time.sleep(random.randint(30, 90))
          EOF
          python3 /app/producer.py
        resources:
          requests:
            memory: "64Mi"
            cpu: "25m"
          limits:
            memory: "128Mi"
            cpu: "100m"